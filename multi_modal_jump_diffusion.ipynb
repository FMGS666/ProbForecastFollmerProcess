{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import os \n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ProbForecastFollmerProcess as pffp\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.distributions import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting plotting style and defining the device\n",
    "plt.style.use('ggplot')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_folder = \"./data_store/multi_modal_jump_diffusion\"\n",
    "print('Computing on ' + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting reproducibility\n",
    "reproducible = True\n",
    "SEED = 1024 if reproducible else int(time.time())\n",
    "pffp.utils.ensure_reproducibility(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining number of dimensions\n",
    "dim = 2\n",
    "\n",
    "# defining base gaussian component\n",
    "mean_0 = torch.tensor([5.0, 0.0], device = device)\n",
    "cov_0 = torch.tensor([[1.5, 0.0], [0.0, 0.1]], device = device)\n",
    "\n",
    "# defining number of mixture models\n",
    "K = 5\n",
    "\n",
    "# defining rotation angle\n",
    "theta = torch.tensor([2.0*np.pi/K], device = device)\n",
    "\n",
    "# defining rotation matrix\n",
    "R_theta = lambda k: torch.tensor([[torch.cos(k*theta), -torch.sin(k*theta)], [torch.sin(k*theta), torch.cos(k*theta)]], device = device)\n",
    "rotations = [R_theta(k) for k in range(K)]\n",
    "\n",
    "# defining mixture\n",
    "gmm_params = [{\"mean\": torch.matmul(R, mean_0), \"cov\": torch.matmul(torch.matmul(R, cov_0), R.T)} for R in rotations]\n",
    "gmm_components = [MultivariateNormal(params[\"mean\"], params[\"cov\"]) for params in gmm_params]\n",
    "print(gmm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings of the simulation\n",
    "delta_t = torch.tensor([1e-2], device = device)\n",
    "observation_interval = 50\n",
    "\n",
    "# defining number of observations and iterations\n",
    "num_iters = int(5e6) \n",
    "num_observations = int(num_iters / observation_interval)\n",
    "\n",
    "# rate of the poisson process\n",
    "poisson_rate = torch.tensor([2.0], device = device)\n",
    "\n",
    "# defining the configuration dictionary\n",
    "simulation_conf = {\n",
    "    \"delta_t\": delta_t,\n",
    "    \"observation_interval\": observation_interval,\n",
    "    \"num_iters\": num_iters,\n",
    "    \"num_observations\": num_observations,\n",
    "    \"poisson_rate\": poisson_rate,\n",
    "    \"gmm_components\": gmm_components,\n",
    "    \"R_theta\": R_theta,\n",
    "    \"K\": K,\n",
    "    \"dim\": dim,\n",
    "    \"device\": device\n",
    "}\n",
    "\n",
    "# checking if the simulation data is stored \n",
    "if len(os.listdir(data_folder)) == 0:\n",
    "    print(\"Data not found, running the simulation\")\n",
    "    # simulating the trajectory we will use for training\n",
    "    train_state_store, train_observation_store = pffp.utils.simulate_jump_diffusion(simulation_conf)\n",
    "    # simulating the trajectory we will use for testing\n",
    "    test_state_store, test_observation_store = pffp.utils.simulate_jump_diffusion(simulation_conf)\n",
    "    # saving the simulations so that we won't have to run them again\n",
    "    torch.save(train_state_store, os.path.join(data_folder, \"train_states.pt\"))\n",
    "    torch.save(train_observation_store, os.path.join(data_folder, \"train_observations.pt\"))\n",
    "    torch.save(test_state_store, os.path.join(data_folder, \"test_states.pt\"))\n",
    "    torch.save(test_observation_store, os.path.join(data_folder, \"test_observations.pt\"))\n",
    "else:\n",
    "    print(\"Loading simulation data from disk\")\n",
    "    # loading training states and observations\n",
    "    train_state_store = torch.load(os.path.join(data_folder, \"train_states.pt\"))\n",
    "    train_observation_store = torch.load(os.path.join(data_folder, \"train_observations.pt\"))\n",
    "    # loading testing states and observations\n",
    "    test_state_store = torch.load(os.path.join(data_folder, \"test_states.pt\"))\n",
    "    test_observation_store = torch.load(os.path.join(data_folder, \"test_observations.pt\"))\n",
    "# printing shape of the data\n",
    "print(f\"{train_state_store.shape=}, {train_observation_store.shape=}\")\n",
    "print(f\"{test_state_store.shape=}, {test_observation_store.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining figure and axes\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "# plotting the states of the simulated dynamics\n",
    "x = train_state_store[:,0].detach().cpu()\n",
    "y = train_state_store[:,1].detach().cpu()\n",
    "pffp.utils.plot_density(x, y, fig, axes[0], \"States\")\n",
    "# plotting the observations of the simulated dynamics\n",
    "x = train_observation_store[:,0].detach().cpu()\n",
    "y = train_observation_store[:,1].detach().cpu()\n",
    "pffp.utils.plot_density(x, y, fig, axes[1], \"Observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining lag\n",
    "tau_mul = 1 # 3 4\n",
    "# sampling function for training data\n",
    "get_train_data = lambda N: (i.to(device) for i in pffp.utils.sample_observations(N, train_observation_store, tau_mul))\n",
    "# sampling function for testing data\n",
    "get_test_data = lambda N: (i.to(device) for i in pffp.utils.sample_observations(N, test_observation_store, tau_mul))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining data configurations\n",
    "data = {    \n",
    "    \"train\": get_train_data, \n",
    "    \"test\": get_train_data, \n",
    "}\n",
    "\n",
    "# defining sampling configurations\n",
    "sample = {\n",
    "    \"g\": pffp.interpolant[\"sigma\"],#pffp.utils.g_follmer, \n",
    "    \"N\": 1000\n",
    "}\n",
    "\n",
    "# learning standardization means and standard deviations\n",
    "standardize = True\n",
    "standardization = {\n",
    "    'state_mean': torch.mean(train_observation_store, 0).to(device) if standardize else torch.zeros(dim).to(device),\n",
    "    'state_std': torch.std(train_observation_store, 0).to(device) if standardize else torch.ones(dim).to(device) ,\n",
    "}\n",
    "print(standardization)\n",
    "\n",
    "# defining network configurations\n",
    "net_config = {\n",
    "    \"layers\": [500]*5, \n",
    "    \"standardization\": standardization\n",
    "}\n",
    "\n",
    "# defining state configurations \n",
    "state = {\n",
    "    \"dim\": dim\n",
    "}\n",
    "\n",
    "# defining optimization configurations\n",
    "optim_config = {\n",
    "    'minibatch': 1, \n",
    "    'num_obs_per_batch': 1000, \n",
    "    'num_iterations': 300,\n",
    "    'learning_rate' : 0.001,\n",
    "    'num_mc_samples': 100 \n",
    "}\n",
    "\n",
    "# defining model\n",
    "model = pffp.core.model(data, sample, state, pffp.utils.interpolant, pffp.utils.velocity, net_config, device = \"cuda\")\n",
    "\n",
    "# printing model\n",
    "print(model)\n",
    "# training model\n",
    "model.train(optim_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving loss and learning rates\n",
    "losses = model.loss.detach().cpu()\n",
    "lrs = model.lrs.detach().cpu()\n",
    "# defining axes and figure\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "# plotting loss\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].plot(losses)\n",
    "# plotting learning rates\n",
    "axes[1].set_xlabel(\"Iteration\")\n",
    "axes[1].set_ylabel(\"Learning rate\")\n",
    "axes[1].plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling configuration\n",
    "sample_config = {\n",
    "    \"minibatch\": 1, \n",
    "    \"num_obs_per_batch\": 1000,\n",
    "    \"num_samples_per_obs\": 1\n",
    "}\n",
    "\n",
    "# running sampling\n",
    "(X0, X1), samples = model.sample(sample_config)\n",
    "print(f\"{X0.shape=}, {X1.shape=}, {samples.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# retrieving the data\n",
    "x_hat, y_hat = samples[0, :, 0].detach().cpu(), samples[0, :, 1].detach().cpu()\n",
    "x0, y0 = X0[:, 0].detach().cpu(), X0[:, 1].detach().cpu()\n",
    "x1, y1 = X1[:, 0].detach().cpu(), X1[:, 1].detach().cpu()\n",
    "\n",
    "# defining axes and figure\n",
    "fig, axes = plt.subplots(1, 3)\n",
    "\n",
    "# plotting the distributions\n",
    "pffp.utils.plot_density(x0, y0, fig, axes[0], title = \"initial states\", bins = 50)\n",
    "pffp.utils.plot_density(x1, y1, fig, axes[1], title = \"next states\", bins = 50)\n",
    "pffp.utils.plot_density(x_hat, y_hat, fig, axes[2], title = \"sampled states\", bins = 50)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to angular coordinates\n",
    "theta_0 = pffp.utils.vec2angle(x0,y0)\n",
    "theta_1 = pffp.utils.vec2angle(x1,y1)\n",
    "theta_hat = pffp.utils.vec2angle(x_hat,y_hat)\n",
    "\n",
    "# defining figure and axes\n",
    "fig, axes = plt.subplots(1, 3, figsize = (10, 5))\n",
    "\n",
    "# plotting the angular distributions\n",
    "hist0 = axes[0].hist(theta_0, density = True, bins = 200)\n",
    "hist1 = axes[1].hist(theta_1, density = True, bins = 200)\n",
    "hist_hat = axes[2].hist(theta_hat, density = True, bins = 200)\n",
    "\n",
    "# showing the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoregressive sampling configuration\n",
    "sample_config = {\n",
    "    \"minibatch\": 1, \n",
    "    \"num_obs_per_batch\": 1,\n",
    "    \"num_samples_per_obs\": 1,\n",
    "    \"ar_steps\": 100\n",
    "}\n",
    "\n",
    "# running autoregressive sampling\n",
    "X0, ar_samples = model.sample_autoregressive(sample_config)\n",
    "print(f\"{X0.shape=}, {ar_samples.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# retrieving the data\n",
    "x_hat, y_hat = ar_samples[:, 0, 0].detach().cpu(), ar_samples[:, 0, 1].detach().cpu()\n",
    "\n",
    "# defining axes and figure\n",
    "fig, axes = plt.subplots()\n",
    "\n",
    "# plotting the distributions\n",
    "pffp.utils.plot_density(x_hat, y_hat, fig, axes, title = \"sampled states\", bins = 50)\n",
    "axes.scatter(X0[0, 0].detach().cpu(), X0[0, 1].detach().cpu(), color = \"red\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
