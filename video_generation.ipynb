{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic libraries\n",
    "import os\n",
    "import gc \n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# library for image augmentation\n",
    "from albumentations import Compose, Resize\n",
    "\n",
    "# library for unet model\n",
    "from denoising_diffusion_pytorch import Unet\n",
    "\n",
    "# library for VQGAN encode/decoder\n",
    "from river.model import Model\n",
    "from river.lutils.configuration import Configuration\n",
    "\n",
    "# library for stochastic interpolators with follmer processes\n",
    "import ProbForecastFollmerProcess as pffp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting plotting style and defining the device\n",
    "plt.style.use('ggplot')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_folder = \"./store/multi_modal_jump_diffusion\"\n",
    "print('Computing on ' + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting reproducibility\n",
    "reproducible = True\n",
    "SEED = 1024 if reproducible else int(time.time())\n",
    "pffp.utils.ensure_reproducibility(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data paths\n",
    "# defining root to CLEVREV data\n",
    "CLEVREV_root = \"store/video_generation/data/CLEVREV\"\n",
    "# defining base folder of CLEVREV train, validation and test data\n",
    "CLEVREV_train_folder = os.path.join(CLEVREV_root, \"video_train\")\n",
    "CLEVREV_validation_folder = os.path.join(CLEVREV_root, \"video_validation\")\n",
    "CLEVREV_test_folder = os.path.join(CLEVREV_root, \"video_test\")\n",
    "# defining video folders of CLEVEREV train, validation and test data\n",
    "CLEVREV_train_video_folders = [os.path.join(CLEVREV_train_folder, video_folder) for video_folder in os.listdir(CLEVREV_train_folder)]\n",
    "CLEVREV_validation_video_folders = [os.path.join(CLEVREV_validation_folder, video_folder) for video_folder in os.listdir(CLEVREV_validation_folder)]\n",
    "CLEVREV_test_video_folders = [os.path.join(CLEVREV_test_folder, video_folder) for video_folder in os.listdir(CLEVREV_test_folder)]\n",
    "# defining video files for each of the CLEVEREV train, validation and test data\n",
    "CLEVRER_train_video_paths = [os.path.join(video_folder, mp4_file) for video_folder in CLEVREV_train_video_folders for mp4_file in os.listdir(video_folder)]\n",
    "CLEVRER_validation_video_paths = [os.path.join(video_folder, mp4_file) for video_folder in CLEVREV_validation_video_folders for mp4_file in os.listdir(video_folder)]\n",
    "CLEVRER_test_video_paths = [os.path.join(video_folder, mp4_file) for video_folder in CLEVREV_test_video_folders for mp4_file in os.listdir(video_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset and transforms\n",
    "# defining target frame heght and width\n",
    "target_frame_height = target_frame_width = 128\n",
    "# defining custom train, validation and test augmentations\n",
    "train_augmentations = Compose([Resize(target_frame_height, target_frame_width)])\n",
    "validation_augmentations = Compose([Resize(target_frame_height, target_frame_width)])\n",
    "test_augmentations = Compose([Resize(target_frame_height, target_frame_width)])\n",
    "# defining datasets\n",
    "CLEVREV_train_dataset = pffp.data.VideoDataset(CLEVRER_train_video_paths, augmentations = train_augmentations)\n",
    "CLEVREV_validation_dataset = pffp.data.VideoDataset(CLEVRER_validation_video_paths, augmentations = validation_augmentations)\n",
    "CLEVREV_test_dataset = pffp.data.VideoDataset(CLEVRER_test_video_paths, augmentations = test_augmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model's weights and configuration\n",
    "# defining root to CLEVREV weights\n",
    "CLEVREV_weights_root = \"store/video_generation/weights\"\n",
    "# defining path to VQVAE CLEVREV weights\n",
    "VQVAE_CLEVREV_weights_path = os.path.join(CLEVREV_weights_root, \"vqvae.pth\")\n",
    "# defining path to VQGAN CLEVREV weights\n",
    "VQGAN_CLEVREV_weights_path = os.path.join(CLEVREV_weights_root, \"model.pth\")\n",
    "# defining path to CLEVREV configuration\n",
    "CLEVREV_config_path = \"store/video_generation/configs/clevrer.yaml\"\n",
    "# initializing configurations\n",
    "CLEVEREV_configuration = Configuration(CLEVREV_config_path)\n",
    "# initializing model\n",
    "CLEVREV_model = Model(CLEVEREV_configuration[\"model\"]).to(device)\n",
    "# getting the encoder\n",
    "CLEVREV_VQGAN_encoder = CLEVREV_model.ae.backbone.encoder\n",
    "# getting the decoder\n",
    "CLEVREV_VQGAN_decoder = CLEVREV_model.ae.backbone.decoder\n",
    "# defining latent datasets\n",
    "CLEVREV_latent_train_dataset = pffp.data.VQGANLatentVideoDataset(CLEVREV_train_dataset, CLEVREV_VQGAN_encoder, device)\n",
    "CLEVREV_latent_validation_dataset = pffp.data.VQGANLatentVideoDataset(CLEVREV_validation_dataset, CLEVREV_VQGAN_encoder, device)\n",
    "CLEVREV_latent_test_dataset = pffp.data.VQGANLatentVideoDataset(CLEVREV_test_dataset, CLEVREV_VQGAN_encoder, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the first element of the set of videos and try to train our model\n",
    "# on this reduced example for debugging\n",
    "train_example_video = CLEVREV_latent_train_dataset[0]\n",
    "train_examples_paired_frames = pffp.utils.get_video_dataset_with_random_context(train_example_video, device) \n",
    "len(train_examples_paired_frames), type(train_examples_paired_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining number of input channnels for drift estimator\n",
    "num_channels = 4 # four channels for each of the target, previous and random context frames\n",
    "num_frames = 3\n",
    "input_channels = num_channels*num_frames\n",
    "latent_height = latent_width = 16\n",
    "\n",
    "# defining backbone model\n",
    "backbone = Unet(latent_height, out_dim = num_channels, channels = num_channels, input_channels = input_channels, self_condition=True).to(device)\n",
    "\n",
    "# defining class wrapping around the \n",
    "# unet model needed for handling the \n",
    "# shape of the time feature as for B_Network\n",
    "class UnetWrapper(torch.nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(UnetWrapper, self).__init__()\n",
    "        self.base_model = base_model\n",
    "    \n",
    "    def forward(self, X, t, Xc):\n",
    "        N = X.shape[0]\n",
    "        if len(t.shape) == 0:\n",
    "            t = t.repeat((N))\n",
    "        out = self.base_model(X, t, Xc)\n",
    "        return out\n",
    "\n",
    "# defining wrapper around backbone model\n",
    "backbone = UnetWrapper(backbone)\n",
    "\n",
    "# defining sampling configurations\n",
    "sample = {\n",
    "    \"g\": pffp.interpolant[\"sigma\"], \n",
    "    \"num_euler_steps\": 100\n",
    "}\n",
    "\n",
    "# defining state configurations \n",
    "state = {\n",
    "    \"spatial_dims\": (latent_height, latent_width)\n",
    "}\n",
    "\n",
    "# defining data configurations\n",
    "data = {    \n",
    "    \"train\": train_examples_paired_frames, \n",
    "    \"test\": None, \n",
    "}\n",
    "\n",
    "# defining optimization configurations\n",
    "optim_config = {\n",
    "    'batch_size': 6, \n",
    "    'num_epochs': 10,\n",
    "    'learning_rate' : 2e-4,\n",
    "    'num_mc_samples': 300,\n",
    "    'max_num_grad_steps': 250000  \n",
    "}\n",
    "\n",
    "# defining model\n",
    "model = pffp.model(backbone, data, sample, state, pffp.utils.interpolant, pffp.utils.velocity, optim_config, device = \"cuda\")\n",
    "\n",
    "if 0==0:#\"trained_pffp_model.pt\" not in os.listdir(CLEVREV_weights_root):\n",
    "    # print model and message\n",
    "    print(model)\n",
    "    print(\"checkpoint not found, starting training\")\n",
    "    # training model\n",
    "    model.train()\n",
    "    # saving model state dictionary\n",
    "    torch.save(model.state_dict(), os.path.join(CLEVREV_weights_root, \"trained_pffp_model.pt\"))\n",
    "    # retrieving loss and learning rates\n",
    "    losses = model.loss\n",
    "    lrs = model.lrs\n",
    "    # plotting results\n",
    "    # defining axes and figure\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    # plotting loss\n",
    "    axes[0].set_xlabel(\"Iteration\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].plot(losses)\n",
    "    # plotting learning rates\n",
    "    axes[1].set_xlabel(\"Iteration\")\n",
    "    axes[1].set_ylabel(\"Learning rate\")\n",
    "    axes[1].plot(lrs)\n",
    "else:\n",
    "    # print model and message\n",
    "    print(\"checkpoint found, loading model\")\n",
    "    print(model)\n",
    "    state_dict = torch.load(os.path.join(CLEVREV_weights_root, \"trained_pffp_model.pt\"))\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoregressive sampling configuration\n",
    "sample_config = {\n",
    "    \"num_ar_steps\": 1000\n",
    "}\n",
    "\n",
    "# running autoregressive sampling\n",
    "outout = model.sample_autoregressive(sample_config)\n",
    "starting_point = output[\"initial_condition\"]\n",
    "gt_path = output[\"gt_path\"]\n",
    "ar_samples = output[\"ar_path\"]\n",
    "print(f\"{starting_point.shape=}, {gt_path.shape=}, {ar_samples.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling 1000 observation given the first one\n",
    "# sampling configuration\n",
    "sample_config = {\n",
    "    \"num_samples\": 1000,\n",
    "    \"num_obs\": 1\n",
    "}\n",
    "\n",
    "# running sampling\n",
    "output = model.sample(sample_config)\n",
    "X0 = output[\"current_states\"]\n",
    "X1 = output[\"next_state\"]\n",
    "sample = output[\"sampled_states\"]\n",
    "print(f\"{X0.shape=}, {X1.shape=}, {samples.shape=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
