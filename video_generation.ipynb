{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic libraries\n",
    "import os\n",
    "import gc \n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# library for image augmentation\n",
    "from albumentations import Compose, Resize\n",
    "\n",
    "# library for unet model\n",
    "from denoising_diffusion_pytorch import Unet\n",
    "\n",
    "# library for VQGAN encode/decoder\n",
    "from river.model import Model\n",
    "from river.lutils.configuration import Configuration\n",
    "\n",
    "# library for stochastic interpolators with follmer processes\n",
    "import ProbForecastFollmerProcess as pffp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing on cuda\n"
     ]
    }
   ],
   "source": [
    "# setting plotting style and defining the device\n",
    "plt.style.use('ggplot')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_folder = \"./store/multi_modal_jump_diffusion\"\n",
    "print('Computing on ' + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining class for video dataset\n",
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, mp4_file_list, augmentations = None):\n",
    "        super(VideoDataset, self).__init__()\n",
    "        # list of paths to mp4 files\n",
    "        self.mp4_file_list = mp4_file_list\n",
    "        # (callable) optional albumentations tranforms \n",
    "        self.augmentations = augmentations\n",
    "    \n",
    "    def __len__(self):\n",
    "        # getting number of videos in the underlying list\n",
    "        num_videos = len(self.mp4_file_list)\n",
    "        return num_videos\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # get file name\n",
    "        mp4_file = self.mp4_file_list[idx]\n",
    "        # read video\n",
    "        video, _, _ = torchvision.io.read_video(mp4_file, pts_unit = 'sec') # shape: (num_frames, height, width, num_channels)\n",
    "        # normalize pixels in [0, 1]\n",
    "        video = video / 255 \n",
    "        # apply optional augmentations\n",
    "        # need to apply the augmentations \n",
    "        # for each frame of the video \n",
    "        if self.augmentations:\n",
    "            # initializing the list for storing\n",
    "            # each transformed frame\n",
    "            transformed_frames_store = []\n",
    "            # iterating over each frame of the vide\n",
    "            for frame in video:\n",
    "                # converting frame tensor to np array\n",
    "                # as required by albumentations\n",
    "                frame = frame.numpy()\n",
    "                # applying augmentations on array\n",
    "                frame = self.augmentations(image = frame)[\"image\"]\n",
    "                # converting array back to torch tensor\n",
    "                frame = torch.from_numpy(frame)\n",
    "                # appending transformed frame to the store\n",
    "                transformed_frames_store.append(frame)\n",
    "            # concatenating back the frames over the time dimension\n",
    "            video = torch.stack(transformed_frames_store, dim = 0)\n",
    "        # converting back to shape (num_frames, num_channels, height, width)\n",
    "        video = torch.permute(video, (0, 3, 2, 1))\n",
    "        return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data paths\n",
    "# defining root to CLEVREV data\n",
    "CLEVREV_root = \"store/video_generation/data/CLEVREV\"\n",
    "# defining base folder of CLEVREV train, validation and test data\n",
    "CLEVREV_train_folder = os.path.join(CLEVREV_root, \"video_train\")\n",
    "CLEVREV_validation_folder = os.path.join(CLEVREV_root, \"video_validation\")\n",
    "CLEVREV_test_folder = os.path.join(CLEVREV_root, \"video_test\")\n",
    "# defining video folders of CLEVEREV train, validation and test data\n",
    "CLEVREV_train_video_folders = [os.path.join(CLEVREV_train_folder, video_folder) for video_folder in os.listdir(CLEVREV_train_folder)]\n",
    "CLEVREV_validation_video_folders = [os.path.join(CLEVREV_validation_folder, video_folder) for video_folder in os.listdir(CLEVREV_validation_folder)]\n",
    "CLEVREV_test_video_folders = [os.path.join(CLEVREV_test_folder, video_folder) for video_folder in os.listdir(CLEVREV_test_folder)]\n",
    "# defining video files for each of the CLEVEREV train, validation and test data\n",
    "CLEVRER_train_video_paths = [os.path.join(video_folder, mp4_file) for video_folder in CLEVREV_train_video_folders for mp4_file in os.listdir(video_folder)]\n",
    "CLEVRER_validation_video_paths = [os.path.join(video_folder, mp4_file) for video_folder in CLEVREV_validation_video_folders for mp4_file in os.listdir(video_folder)]\n",
    "CLEVRER_test_video_paths = [os.path.join(video_folder, mp4_file) for video_folder in CLEVREV_test_video_folders for mp4_file in os.listdir(video_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset and transforms\n",
    "# defining target frame heght and width\n",
    "target_frame_height = target_frame_width = 128\n",
    "# defining custom train, validation and test augmentations\n",
    "train_augmentations = Compose([Resize(target_frame_height, target_frame_width)])\n",
    "validation_augmentations = Compose([Resize(target_frame_height, target_frame_width)])\n",
    "test_augmentations = Compose([Resize(target_frame_height, target_frame_width)])\n",
    "# defining datasets\n",
    "CLEVREV_train_dataset = VideoDataset(CLEVRER_train_video_paths, augmentations = train_augmentations)\n",
    "CLEVREV_validation_dataset = VideoDataset(CLEVRER_validation_video_paths, augmentations = validation_augmentations)\n",
    "CLEVREV_test_dataset = VideoDataset(CLEVRER_test_video_paths, augmentations = test_augmentations)\n",
    "# defining dictionary with datasets\n",
    "CLEVREV_datasets = {\n",
    "    \"train\": CLEVREV_train_dataset,\n",
    "    \"validation\": CLEVREV_validation_dataset,\n",
    "    \"test\": CLEVREV_test_dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model's weights and configuration\n",
    "# defining root to CLEVREV weights\n",
    "CLEVREV_weights_root = \"store/video_generation/weights\"\n",
    "# defining path to VQVAE CLEVREV weights\n",
    "VQVAE_CLEVREV_weights_path = os.path.join(CLEVREV_weights_root, \"vqvae.pth\")\n",
    "# defining path to VQGAN CLEVREV weights\n",
    "VQGAN_CLEVREV_weights_path = os.path.join(CLEVREV_weights_root, \"model.pth\")\n",
    "# defining path to CLEVREV configuration\n",
    "CLEVREV_config_path = \"store/video_generation/configs/clevrer.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and configurations\n",
    "# initializing configurations\n",
    "CLEVEREV_configuration = Configuration(CLEVREV_config_path)\n",
    "# initializing model\n",
    "CLEVREV_model = Model(CLEVEREV_configuration[\"model\"]).to(device)\n",
    "# getting the encoder\n",
    "CLEVREV_VQGAN_encoder = CLEVREV_model.ae.backbone.encoder\n",
    "# getting the decoder\n",
    "CLEVREV_VQGAN_decoder = CLEVREV_model.ae.backbone.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the VQGAN latent video dataset\n",
    "class VQGANLatentVideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset, encoder, device):\n",
    "        super(VQGANLatentVideoDataset, self).__init__()\n",
    "        self.base_dataset = base_dataset # instance of VideoDataset class\n",
    "        self.encoder = encoder # instance of river.model.encoder.Encoder class\n",
    "        self.device = device # device on which the encoder is run\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # getting video and moving it to device\n",
    "        video = self.base_dataset[idx].to(self.device)\n",
    "        # encoding latent video, detaching it from computation graph \n",
    "        # as we don't need the gradients wrt the encoder and moving\n",
    "        # it to cpu for saving up gpu memory\n",
    "        latent_video = self.encoder(video).detach().cpu()\n",
    "        # deleting unused video and cleaning up memory and gpu cache\n",
    "        del video\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return latent_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining latent datasets\n",
    "CLEVREV_latent_datasets = {dataset_name: VQGANLatentVideoDataset(dataset, CLEVREV_VQGAN_encoder, device) for dataset_name, dataset in CLEVREV_datasets.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
