{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic libraries\n",
    "import os\n",
    "import gc \n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# library for image augmentation\n",
    "from albumentations import Compose, Resize\n",
    "\n",
    "# library for unet model\n",
    "from denoising_diffusion_pytorch import Unet\n",
    "\n",
    "# library for VQGAN encode/decoder\n",
    "from river.model import Model\n",
    "from river.lutils.configuration import Configuration\n",
    "\n",
    "# library for stochastic interpolators with follmer processes\n",
    "import ProbForecastFollmerProcess as pffp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting plotting style and defining the device\n",
    "plt.style.use('ggplot')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_folder = \"./store/multi_modal_jump_diffusion\"\n",
    "print('Computing on ' + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting reproducibility\n",
    "reproducible = True\n",
    "SEED = 1024 if reproducible else int(time.time())\n",
    "pffp.utils.ensure_reproducibility(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data paths\n",
    "# defining root to CLEVREV data\n",
    "CLEVREV_root = \"store/video_generation/data/CLEVREV\"\n",
    "# defining base folder of CLEVREV train, validation and test data\n",
    "CLEVREV_train_folder = os.path.join(CLEVREV_root, \"video_train\")\n",
    "CLEVREV_validation_folder = os.path.join(CLEVREV_root, \"video_validation\")\n",
    "CLEVREV_test_folder = os.path.join(CLEVREV_root, \"video_test\")\n",
    "# defining video folders of CLEVEREV train, validation and test data\n",
    "CLEVREV_train_video_folders = [os.path.join(CLEVREV_train_folder, video_folder) for video_folder in os.listdir(CLEVREV_train_folder)]\n",
    "CLEVREV_validation_video_folders = [os.path.join(CLEVREV_validation_folder, video_folder) for video_folder in os.listdir(CLEVREV_validation_folder)]\n",
    "CLEVREV_test_video_folders = [os.path.join(CLEVREV_test_folder, video_folder) for video_folder in os.listdir(CLEVREV_test_folder)]\n",
    "# defining video files for each of the CLEVEREV train, validation and test data\n",
    "CLEVRER_train_video_paths = [os.path.join(video_folder, mp4_file) for video_folder in CLEVREV_train_video_folders for mp4_file in os.listdir(video_folder)]\n",
    "CLEVRER_validation_video_paths = [os.path.join(video_folder, mp4_file) for video_folder in CLEVREV_validation_video_folders for mp4_file in os.listdir(video_folder)]\n",
    "CLEVRER_test_video_paths = [os.path.join(video_folder, mp4_file) for video_folder in CLEVREV_test_video_folders for mp4_file in os.listdir(video_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset and transforms\n",
    "# defining target frame heght and width\n",
    "target_frame_height = target_frame_width = 128\n",
    "# defining custom train, validation and test augmentations\n",
    "train_augmentations = Compose([Resize(target_frame_height, target_frame_width)])\n",
    "validation_augmentations = Compose([Resize(target_frame_height, target_frame_width)])\n",
    "test_augmentations = Compose([Resize(target_frame_height, target_frame_width)])\n",
    "# defining datasets\n",
    "CLEVREV_train_dataset = pffp.data.VideoDataset(CLEVRER_train_video_paths, augmentations = train_augmentations)\n",
    "CLEVREV_validation_dataset = pffp.data.VideoDataset(CLEVRER_validation_video_paths, augmentations = validation_augmentations)\n",
    "CLEVREV_test_dataset = pffp.data.VideoDataset(CLEVRER_test_video_paths, augmentations = test_augmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model's weights and configuration\n",
    "# defining root to CLEVREV weights\n",
    "CLEVREV_weights_root = \"store/video_generation/weights\"\n",
    "# defining path to VQVAE CLEVREV weights\n",
    "VQVAE_CLEVREV_weights_path = os.path.join(CLEVREV_weights_root, \"vqvae.pth\")\n",
    "# defining path to VQGAN CLEVREV weights\n",
    "VQGAN_CLEVREV_weights_path = os.path.join(CLEVREV_weights_root, \"model.pth\")\n",
    "# defining path to CLEVREV configuration\n",
    "CLEVREV_config_path = \"store/video_generation/configs/clevrer.yaml\"\n",
    "# initializing configurations\n",
    "CLEVEREV_configuration = Configuration(CLEVREV_config_path)\n",
    "# initializing model\n",
    "CLEVREV_model = Model(CLEVEREV_configuration[\"model\"]).to(device)\n",
    "# getting the encoder and setting it to inference mode\n",
    "CLEVREV_VQGAN_encoder = CLEVREV_model.ae.backbone.encoder\n",
    "CLEVREV_VQGAN_encoder.eval()\n",
    "# getting the decoder and setting it to inference mode\n",
    "CLEVREV_VQGAN_decoder = CLEVREV_model.ae.backbone.decoder\n",
    "CLEVREV_VQGAN_decoder.eval()\n",
    "# defining latent datasets\n",
    "CLEVREV_latent_train_dataset = pffp.data.VQGANLatentVideoDataset(CLEVREV_train_dataset, CLEVREV_VQGAN_encoder, device)\n",
    "CLEVREV_latent_validation_dataset = pffp.data.VQGANLatentVideoDataset(CLEVREV_validation_dataset, CLEVREV_VQGAN_encoder, device)\n",
    "CLEVREV_latent_test_dataset = pffp.data.VQGANLatentVideoDataset(CLEVREV_test_dataset, CLEVREV_VQGAN_encoder, device)\n",
    "print(f\"Number of train videos {len(CLEVREV_latent_train_dataset)}\")\n",
    "print(f\"Number of validation videos {len(CLEVREV_latent_validation_dataset)}\")\n",
    "print(f\"Number of test videos {len(CLEVREV_latent_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining class for concatenated video dataset\n",
    "# needed to be able to train over all frames of each \n",
    "# of the 10000 videos of the CLEVREV train dataset\n",
    "class ConcatenatedVideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_datasets, device):\n",
    "        super(ConcatenatedVideoDataset, self).__init__()\n",
    "        # the class holding the data of each video\n",
    "        # instance of VQGANLatentVideoDataset\n",
    "        # each of its element will be a full video\n",
    "        # but we want to be able to sample training paired frames\n",
    "        # from all the videos.\n",
    "        self.base_datasets = base_datasets\n",
    "        # initializing the device attribute \n",
    "        self.device = device\n",
    "        # getting the number of videos in the dataset\n",
    "        self.num_videos = len(self.base_datasets)\n",
    "        # getting the number of frames per video\n",
    "        # It assumes that all the videos will have the same number of frames.\n",
    "        self.num_frames_per_video = len(self.base_datasets[0]) - 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_videos*self.num_frames_per_video\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # getting video index\n",
    "        video_idx = idx // self.num_frames_per_video\n",
    "        # getting frame index\n",
    "        frame_idx = idx % self.num_frames_per_video\n",
    "        # getting the video\n",
    "        video = self.base_datasets[video_idx]\n",
    "        # creating dataset with random context\n",
    "        video_paired_frames = pffp.utils.get_video_dataset_with_random_context(pffp.data.LaggedDatasetWithRandomContext, video, self.device) \n",
    "        # getting the frame data from the current video\n",
    "        frame_data = video_paired_frames[frame_idx]\n",
    "        return frame_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining number of input channnels for drift estimator\n",
    "num_latent_channels = 4 # four channels for each of the target, previous and random context frames\n",
    "num_frames = 3\n",
    "input_channels = num_latent_channels*num_frames\n",
    "latent_height = latent_width = 16\n",
    "\n",
    "# defining backbone model\n",
    "backbone = Unet(\n",
    "    latent_height, \n",
    "    out_dim = num_latent_channels, \n",
    "    channels = num_latent_channels, \n",
    "    input_channels = input_channels, \n",
    "    self_condition=True\n",
    ").to(device)\n",
    "\n",
    "# defining class wrapping around the \n",
    "# unet model needed for handling the \n",
    "# shape of the time feature as for B_Network\n",
    "class UnetWrapper(torch.nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(UnetWrapper, self).__init__()\n",
    "        self.base_model = base_model\n",
    "    \n",
    "    def forward(self, X, t, Xc):\n",
    "        N = X.shape[0]\n",
    "        if len(t.shape) == 0:\n",
    "            t = t.repeat((N))\n",
    "        out = self.base_model(X, t, Xc)\n",
    "        return out\n",
    "\n",
    "# defining wrapper around backbone model\n",
    "backbone = UnetWrapper(backbone)\n",
    "\n",
    "# defining sampling configurations\n",
    "sample = {\n",
    "    \"g\": pffp.interpolant[\"sigma\"], \n",
    "    \"num_euler_steps\": 100\n",
    "}\n",
    "\n",
    "# defining state configurations \n",
    "state = {\n",
    "    \"input_dims\": (num_latent_channels, latent_height, latent_width)\n",
    "}\n",
    "\n",
    "# defining data configurations\n",
    "data = {    \n",
    "    \"train\": ConcatenatedVideoDataset(CLEVREV_latent_train_dataset, device), \n",
    "    \"test\": ConcatenatedVideoDataset(CLEVREV_latent_validation_dataset, device), \n",
    "}\n",
    "\n",
    "# defining optimization configurations\n",
    "optim_config = {\n",
    "    'batch_size': 6, \n",
    "    'num_epochs': 5,\n",
    "    'learning_rate' : 2e-4,\n",
    "    'num_mc_samples': 300,\n",
    "    'max_num_grad_steps': 250000,\n",
    "    'constant_lr': True  \n",
    "}\n",
    "\n",
    "# defining model\n",
    "model = pffp.model(\n",
    "    backbone, \n",
    "    data, \n",
    "    sample, \n",
    "    state, \n",
    "    pffp.utils.interpolant, \n",
    "    pffp.utils.velocity, \n",
    "    optim_config, \n",
    "    device = \"cuda\", \n",
    "    verbose = 2, \n",
    "    debug = False,\n",
    "    random_ar_context = True\n",
    ")\n",
    "\n",
    "if \"trained_pffp_model.pt\" not in os.listdir(CLEVREV_weights_root):\n",
    "    # print model and message\n",
    "    print(model)\n",
    "    print(\"checkpoint not found, starting training\")\n",
    "    # training model\n",
    "    model.train()\n",
    "    # saving model state dictionary\n",
    "    torch.save(model.state_dict(), os.path.join(CLEVREV_weights_root, \"trained_pffp_model.pt\"))\n",
    "    # retrieving loss and learning rates\n",
    "    losses = model.loss_values\n",
    "    lrs = model.lrs_values\n",
    "    # plotting results\n",
    "    # defining axes and figure\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    # plotting loss\n",
    "    axes[0].set_xlabel(\"Iteration\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].plot(losses)\n",
    "    # plotting learning rates\n",
    "    axes[1].set_xlabel(\"Iteration\")\n",
    "    axes[1].set_ylabel(\"Learning rate\")\n",
    "    axes[1].plot(lrs)\n",
    "else:\n",
    "    # print model and message\n",
    "    print(\"checkpoint found, loading model\")\n",
    "    print(model)\n",
    "    state_dict = torch.load(os.path.join(CLEVREV_weights_root, \"trained_pffp_model.pt\"))\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# autoregressive sampling configuration\n",
    "sample_config = {\n",
    "    \"num_ar_steps\": 14,\n",
    "    \"starting_idx\": 10\n",
    "}\n",
    "\n",
    "# running autoregressive sampling\n",
    "output = model.sample_autoregressive(sample_config, train = False)\n",
    "starting_point = output[\"initial_condition\"]\n",
    "gt_path = output[\"gt_path\"]\n",
    "ar_latent_samples = output[\"ar_path\"]\n",
    "# decoding the latent autoregressive samples \n",
    "# while moving the tensors where they are hold to device\n",
    "ar_decoded_samples = CLEVREV_VQGAN_decoder(ar_latent_samples.to(device))\n",
    "print(f\"{starting_point.shape=}, {gt_path.shape=}, {ar_latent_samples.shape=}, {ar_decoded_samples.shape=}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# showing an example frame\n",
    "sampled_frames = (ar_decoded_samples - torch.min(ar_decoded_samples))/(torch.max(ar_decoded_samples) - torch.min(ar_decoded_samples))\n",
    "sampled_frames = sampled_frames.detach().cpu()\n",
    "sampled_frames = torch.permute(sampled_frames, (0, 2, 3, 1))\n",
    "plt.imshow(sampled_frames[])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# sampling 1000 observation given the first one\n",
    "# sampling configuration\n",
    "sample_config = {\n",
    "    \"num_samples\": 14,\n",
    "    \"num_obs\": 1\n",
    "}\n",
    "\n",
    "# running sampling\n",
    "output = model.sample(sample_config)\n",
    "X0 = output[\"current_states\"]\n",
    "X1 = output[\"next_state\"]\n",
    "sample = output[\"sampled_states\"]\n",
    "print(f\"{X0.shape=}, {X1.shape=}, {samples.shape=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
